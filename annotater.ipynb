{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matsumoton/miniconda3/envs/ecoute/lib/python3.11/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit\n",
      "/Users/matsumoton/miniconda3/envs/ecoute/lib/python3.11/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#huggingface\n",
    "#\n",
    "AUDIO_FILE = \"output-audio.aac\"\n",
    "\n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"large\")\n",
    "result = model.transcribe(AUDIO_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'ecoute' requires notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import dill\n",
    "result = dill.load(open(\"transcriber.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transcript', 'suggestion', 'summarizations'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Courser\\n\\nWe'll see you in two guys.\\n\\nThank you, everyone. Bye.\\n\\nAnd then we'll continue to make progress on this, so.\\n\\ngoals and clarity on the end goals like that paper you know by mid 2024 and then uh the fight the second phase i mean i like that the clarity of the two faces i like that all thanks\\n\\nwe got, we got really good, uh, uh,\\n\\nOkay, thank you. Alright guys, so we'll call the meeting in for next week. We'll have a nice demo and everybody will be happy. And then we'll, I guess we'll have some extraction items, I guess, before the end of the year. Sounds great.\\n\\nOkay, okay makes sense\\n\\nOkay, the entities are all in there, but just a relationship only covers 25% all right\\n\\nOkay, we'll evaluate it I guess yeah\\n\\ntoo. But if it's a easy inclusion, then it will be nice to include it.\\n\\nWill it create overhead maybe on the system? Is it possible to have it? If not, then don't worry. We won't worry about it.\\n\\nAlso, KB also campaigns this and that.\\n\\nHas no connected notes. Yeah. Yeah, it's not connected. But just the System know that\\n\\ninclude them as entities or has\\n\\nNot in the multi-hub or anything, but just\\n\\n75% not has certain relationship.\\n\\nIs it possible to just include the...\\n\\nSorry, I have to take a step back is it\\n\\nBut is it, I'm just, I think we already.\\n\\nunless you tell us otherwise. I mean, if you find something else, a more connection, so there is a mistake or anything on your analysis, then let us know right away and then we can include more notes. But for now, that will be our universe, you know, that 25%.\\n\\nThat will be our knowledge for now.\\n\\nWe're gonna work on that 25% basically and then just go on with that, right?\\n\\nJune, Nick, I mean, I guess.\\n\\nby three or Natalie or she jay\\n\\nIs there is there any anything you need from\\n\\nOkay.\\n\\nhave some demo of the prototype.\\n\\nYeah, Miguel, it's a good plan. Yeah, it'll be nice that we can\\n\\nto be finished by early 2024. Just a heads up.\\n\\nthe master's work which is the GWAS data analysis also needs\\n\\nSo that's one thing. Also, it relates to.\\n\\nwhich has the methodology development on top of ALZKB.\\n\\nby mid-year of 2024, we have some...\\n\\nnext year, early next year, that hopefully.\\n\\nnext month and then we can work on phase two.\\n\\nyear of 2024, so it will be great that we have some prototype.\\n\\nThe paper will be out by mid...\\n\\nper, that's the NIH grant.\\n\\nJust a quick update about the timeline that we are supposed to send out a paper.\\n\\nfor the DS team, I guess.\\n\\nthen we'll get some input from there. Is there is there anything else that you want to add?\\n\\nAnd hopefully we can come up with it and then I'll call the meeting and then we'll just show them on probably a 30-minute thing, right?\\n\\ndemo maybe late next week so that we can we can demonstrate to to the DS team and to Paul and the rest of the team. So we're going to work we're going to work on that.\\n\\nI mean, one of the action items, one of the commitments is so that we can have a.\\n\\nI mean we're like getting almost to the top of the hour. I would like to get\\n\\nYeah, but today at two we meet, right?\\n\\nYeah, because this is more software-related. Yeah, yeah, yeah.\\n\\nThing right the production ready thing. So those are the things I have to discuss. I have like some questions maybe we'll push that to the next meeting, but Yeah, this is but yeah, cuz I wanted\\n\\none thing, but what I'm trying to build now is more the production.\\n\\nwhat you're saying. Yeah. Yeah. So, so yeah. So, but okay. The demo is.\\n\\nThese things, right? But I was a little confused when you guys said that the website was already pretty much ready, and you guys want to demo it already.\\n\\nDeployment once we configure those things. This is the thing we're just talking about right now. I was just making notes on this yesterday defining the\\n\\nif we should do it locally, that's what's here. We still need to evaluate all of that. And then for model deployment, I mean, I'm sorry for, yeah, for LLMD.\\n\\nAnd the one that you guys mentioned about the API wrappers, maybe we don't know if we should use HuggingFace yet, or.\\n\\nstuff for the connection between the front end and the back end.\\n\\nwhat tools and things I was trying to use, right? So I was considering different databases and I'm looking for like PostgreSQL is probably what I wanna use. And then we still have to design the endpoints and maybe use some events with WebSockets and stuff.\\n\\nI've created inside the OSKB site repository, you guys still have access to this, so I was just writing my notes on...\\n\\nJune also said that you I mean you guys also said you guys have it ready, but I was a little Confused by that because I mean I was just like I said making plans for the back end the back\\n\\nYeah, and then one last thing I wanted to mention is about the Oscar Beach at but thing I know\\n\\nthat i think we really need to plan out and design though right i don't i don't know i don't know\\n\\nIt's uh, yeah, that's it. That's the thing. Those are considerations\\n\\nYeah, we need to probably consider It's part of the fine-tuning, I guess, right? We're domesticating this\\n\\nCussing everybody out, you know?\\n\\nyou know what one of the talks I know the the symposium one of the talks one of the uh person one of the the speakers there was saying about toxicity I don't know what methods there are for removing toxicity on models or whatever but that's I think that's something we may need to consider because if we're going to plug this into a robot the robot's going to be\\n\\nYeah, and and and you know like the thing that you guys mentioned about the Lady Gaga thing that wasn't appropriate It was random. It was random, but but that actually\\n\\nFocus their efforts on right? I know you I mean, it sounds like that's what we're talking about But when you guys say that you guys have everything ready, I don't see the methodology though. I mean i'm trying I'm trying to figure that out. How do we Go from there. Do we still have to build that out?\\n\\nSo we can start deciding which model to use.\\n\\ncome up with a score so that we can do we can feed it to the leaderboard right so the leaderboard\\n\\nso that I could get those. At the end, I was gonna try to.\\n\\nor evaluating the strings, but then for each model, let's say you give me 10 models, I was going to start just batch processing them.\\n\\nWell, I don't know if how good it was going to be but I was going to try to use the embeddings or the gpt4\\n\\nsome APIs and I know we discussed human evaluations right I know June has that website where with the slider where people can select which model is better right so that was one human evaluation portion and then we can use\\n\\nDo we still need to put together the processes for evaluating this because my planning was I was gonna use\\n\\nYou know, the model fine-tuning is what I know that June has been working on a lot, but then the model performance that we were going to evaluate...\\n\\nthe one I'm moving right now. And the one on this frame here is the one that I think we're discussing right now, which.\\n\\nOne will be the website, which is on the right here.\\n\\nhere so the components of the project right we have one come I mean we have two different\\n\\nlike today's software meeting a little bit I guess because if we keep talking about this one because I was trying to like put something\\n\\nBecause I was I was I wanted to make sure and this is gonna be like a precursor\\n\\nscreenshot my screen really quick.\\n\\nBecause I know we just have five minutes, but can I can I just share once?\\n\\nJust by visually looking at it?\\n\\nI guess, how are you guys evaluating those things?\\n\\non the system that will allow us to fine-tune other models. Is that what I'm getting?\\n\\nfor this exercise and to do development, I guess.\\n\\ndifferent models, right? I mean, but you're using Acuna for\\n\\nI guess the system will allow you to test.\\n\\nThe best models that came out of it was\\n\\nI see. Yeah. Instead of, like, having to-\\n\\nevaluations and things but it sounds like you guys keep saying that you guys have stuff ready but you know i'm not sure because i was just in the planning stages and now you're you guys are already jumping to selecting a model already when i don't see the\\n\\nThere's some things that I because I was already making some plans for certain\\n\\nsticking just with because I thought we were gonna do a little bit more analysis on that I thought we were going to actually I mean I don't know I think at today's meeting at the software meeting we could discuss some more software related things because I think you know\\n\\nSo wait, so you guys are deciding already on-\\n\\nif we change the grade or certain hyper parameters.\\n\\nwith Fakuna then what if we do this?\\n\\nHakuna, so right now we're on we're\\n\\nof casually went through the models and the best.\\n\\nJun's already just kind of like...\\n\\nIf not, then once you deploy this, then we could, everybody could contribute to this fine-tuning process, right?\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result['transcript'].split('\\n\\n')).to_csv(\"transcript.csv\")\n",
    "pd.DataFrame(result['suggestion']).to_csv(\"suggestion.csv\")\n",
    "pd.DataFrame(result['summarizations']).to_csv(\"summarizations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_file(AUDIO_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'segments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m transcript \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39msegments\u001b[39m\u001b[39m\"\u001b[39m])):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# result[\"segments\"][i][\"start\"] = int(result[\"segments\"][i][\"start\"] / 1000)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m# result[\"segments\"][i][\"end\"] = int(result[\"segments\"][i][\"end\"] / 1000)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# print(int(result[\"segments\"][i][\"start\"]), result[\"segments\"][i][\"text\"])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39msegments\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m     transcript\u001b[39m.\u001b[39mappend((\u001b[39mint\u001b[39m(result[\u001b[39m\"\u001b[39m\u001b[39msegments\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m]),result[\u001b[39m\"\u001b[39m\u001b[39msegments\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'segments'"
     ]
    }
   ],
   "source": [
    "transcript = []\n",
    "for i in range(len(result[\"segments\"])):\n",
    "    # result[\"segments\"][i][\"start\"] = int(result[\"segments\"][i][\"start\"] / 1000)\n",
    "    # result[\"segments\"][i][\"end\"] = int(result[\"segments\"][i][\"end\"] / 1000)\n",
    "    # print(int(result[\"segments\"][i][\"start\"]), result[\"segments\"][i][\"text\"])\n",
    "    print(result[\"segments\"][i][\"text\"])\n",
    "    transcript.append((int(result[\"segments\"][i][\"start\"]),result[\"segments\"][i][\"text\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos_people",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Jul  5 2023, 15:34:07) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0d70b8fac395bbb16ce3e9dce3a8b2fab7d9d5b11d92cb5441c7849ecc8e486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
